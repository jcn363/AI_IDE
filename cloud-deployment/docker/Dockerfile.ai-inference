# Multi-stage build for AI Inference Engine container
FROM rust:1.91.0-slim as builder

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    curl \
    pkg-config \
    libssl-dev \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /usr/src/rust-ai-ide

# Copy workspace files
COPY . .

# Build the AI inference crate specifically (release mode)
RUN cargo build --release --package rust-ai-ide-ai-inference --bin ai-inference-engine || \
    cargo build --release --package rust-ai-ide-ai-inference

# Runtime stage (minimal)
FROM debian:bookworm-slim

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    ca-certificates \
    curl \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies for AI/ML with GPU support if needed
# Use CPU version by default, GPU version can be installed via feature flag
ARG USE_GPU=false
RUN if [ "$USE_GPU" = "true" ]; then \
        pip3 install --break-system-packages requests numpy torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121; \
    else \
        pip3 install --break-system-packages requests numpy torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu; \
    fi

# Install CUDA runtime if GPU support is enabled
RUN if [ "$USE_GPU" = "true" ]; then \
        apt-get update && apt-get install -y \
        nvidia-cuda-toolkit \
        && rm -rf /var/lib/apt/lists/*; \
    fi

# Create models directory for downloaded models
RUN mkdir -p /app/models && chmod 755 /app/models

# Create non-root user
RUN useradd --create-home --shell /bin/bash ai-user

# Set working directory
WORKDIR /home/ai-user

# Copy binary from builder
COPY --from=builder /usr/src/rust-ai-ide/target/release/ai-inference-engine /usr/local/bin/ai-inference-engine

# Copy configuration
COPY --from=builder /usr/src/rust-ai-ide/cloud-deployment/docker/ai-config.yaml /home/ai-user/config.yaml

# Make models directory accessible
RUN chown -R ai-user:ai-user /home/ai-user

# Switch to non-root user
USER ai-user

# Expose port for AI inference API
EXPOSE 8000

# Volume for persistent models
VOLUME ["/home/ai-user/models"]

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Environment variables
ENV RUST_LOG=info
ENV MODEL_CACHE_DIR=/home/ai-user/models
ENV INFERENCE_PORT=8000

# Entry point
ENTRYPOINT ["/usr/local/bin/ai-inference-engine"]

# Default command
CMD ["--config", "/home/ai-user/config.yaml", "--port", "8000", "--model-dir", "/home/ai-user/models"]